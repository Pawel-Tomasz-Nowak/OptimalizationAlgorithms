{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Adam algorithm for bivariate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.54210661, -0.03406147]), 0.026010558160080955)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Read the dataset, from which we take the features.\n",
    "dataset = pl.read_csv(\"WineQuality.csv\", separator=';', ignore_errors = True)\n",
    "\n",
    "\n",
    "#Select the features for the model.\n",
    "target_var = \"quality\"\n",
    "predictor = \"pH\"\n",
    "\n",
    "\n",
    "class Adam():\n",
    "        \n",
    "    #Note, we assume obj_func takes two, equally-shaped, one-dimensional arrays and two parameters: b0 and b1 (thus n_params = 2)\n",
    "    def __init__(self,  params:np.ndarray,  X: np.ndarray, y: np.ndarray, alpha:float = 0.001, eps: float = 10**(-8)):\n",
    "        assert X.shape[0] == y.shape[0], \"The first dimensions of the arrays should match!\"\n",
    "\n",
    "        assert type(alpha) is float and alpha > 0, \"The alpha parameter needs to be a positive, numerical value.\"\n",
    "\n",
    "    \n",
    "        self.params = params\n",
    "        self.n_params = params.shape[0]\n",
    "        \n",
    "        #Declare the X variable and y variable.\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Define the alpha and eps which stands for: learning-rate and zero-division-preventer respectively.\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "\n",
    "        #Declare the initial state of m and v (first-moment gradient, second-moment gradient).\n",
    "        self.m = np.zeros(self.n_params)\n",
    "        self.v = np.zeros(self.n_params)\n",
    "\n",
    "    def MSE(self, b0: float, b1:float) -> float:\n",
    "        return np.sum((b1*self.X + b0 - self.y)**2)/self.X.shape[0]\n",
    "\n",
    "\n",
    "    def FindGradients(self,h:float) -> np.ndarray:\n",
    "        partial_derivs = np.zeros(self.n_params)\n",
    "        substractor = self.MSE(self.params[0], self.params[1])\n",
    "\n",
    "        partial_derivs[0] = (self.MSE(self.params[0]+h, self.params[1]) - substractor)/h\n",
    "        partial_derivs[1] = (self.MSE(self.params[0], self.params[1]+h) - substractor)/h\n",
    "\n",
    "        return partial_derivs\n",
    "\n",
    "\n",
    "    \n",
    "    def UpdateMoments(self, bet1:float, bet2:float, h:float):\n",
    "        Gradients = self.FindGradients(h)\n",
    "\n",
    "        new_m = (bet1*self.m + (1-bet1)*Gradients)/(1-bet1)\n",
    "        new_v = (bet2*self.v + (1-bet2)*Gradients**2)/(1-bet2)\n",
    "\n",
    "\n",
    "        return new_m, new_v\n",
    "    \n",
    "    def UpdateParameters(self, bet1:float, bet2:float, h:float):\n",
    "        m_corr, v_corr = self.UpdateMoments(bet1, bet2, h)\n",
    "\n",
    "        self.params = self.params - self.alpha * m_corr / ( np.sqrt(v_corr) +self.eps)\n",
    "\n",
    "        return self.params\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def FitTheLineV2(dataset:pl.DataFrame, predictor:str, target_var:str, eps = 0.001) -> tuple[np.ndarray, float]:\n",
    "    \"\"\"The function finds the optimal parameters, b0, b1, so that \n",
    "    the loss function of linear regressionmodel dataset[target_var] = b0 + b1*dataset[predictor] is minimized\"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    B = np.random.uniform(low = -3, high = 3, size = 2)\n",
    "\n",
    "    #Take the X feature and y feature from the dataset. Then, numpyize it!. Afterward, normalize it!\n",
    "    MinMaxScaler_X = MinMaxScaler()\n",
    "    MinMaxScaler_y = MinMaxScaler()\n",
    "\n",
    "    X = MinMaxScaler_X.fit_transform(dataset.select(predictor).to_numpy())\n",
    "    y = MinMaxScaler_y.fit_transform(dataset.select(target_var).to_numpy())\n",
    "\n",
    "\n",
    "    #Declare an instance of the Adam optimizer.\n",
    "    Adamek = Adam(B, X, y)\n",
    "\n",
    "    #The following variable indicates the difference between the new value of the function and the old one.\n",
    "    #If it's  absolute is less than eps, break the loop.\n",
    "    val_change= float('inf')\n",
    "\n",
    "    for i in range(2500):\n",
    "        B_new = Adamek.UpdateParameters(0.9, 0.9, 0.001)\n",
    "\n",
    "        val_change = abs( Adamek.MSE(B_new[0], B_new[1]) - Adamek.MSE(B[0], B[1]))\n",
    "        B = B_new\n",
    "\n",
    "\n",
    "\n",
    "    return B, Adamek.MSE(B[0], B[1])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "FitTheLineV2(dataset, predictor, target_var, eps = 0.0005)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00194853 -0.00194853]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Define some three-variable function\n",
    "obj_func = lambda X: X[0]**2+X[1]**2\n",
    "\n",
    "\n",
    "\n",
    "class Adamv2():\n",
    "\n",
    "    def __init__(self, params:np.ndarray, alpha: float, eps: float = 10**(-8)):\n",
    "        self.params = params\n",
    "        self.n_params = params.shape[0]\n",
    "\n",
    "        #Alpha is a learning rate.\n",
    "        self.alpha = alpha\n",
    "        #epsilon will be util when updating the parameters. It prevents deviding by zero.\n",
    "        self.eps = eps\n",
    "\n",
    "\n",
    "        self.m = np.zeros(self.n_params)\n",
    "        self.v = np.zeros(self.n_params)\n",
    "\n",
    "    \n",
    "\n",
    "    def FindGradient(self, params:np.ndarray, h:float):\n",
    "        #The substractor is a term which always in appers in the quotient difference of every parameters.\n",
    "        substractor = obj_func(params)\n",
    "\n",
    "        #Define the gradient-vector where all the values of partial derivates will be stored.\n",
    "        Gradient = np.empty(self.n_params)\n",
    "\n",
    "\n",
    "        for i in range(self.n_params):\n",
    "            offset = np.zeros(self.n_params)\n",
    "            offset[i] += h\n",
    "\n",
    "            Gradient[i] = (obj_func(params + offset) - substractor)/h\n",
    "\n",
    "        del substractor\n",
    "        return Gradient\n",
    "\n",
    "\n",
    "\n",
    "    def UpdateParams(self, B:np.ndarray):\n",
    "        #Find the gradient of the objective function.\n",
    "        Gradient = self.FindGradient(self.params, 0.001)\n",
    "\n",
    "\n",
    "        #Update the first-moment gradient and second-moment gradient respectively.\n",
    "        self.m = (B[0]*self.m + (1-B[0])*Gradient)/(1-B[0])\n",
    "        self.v = (B[1]*self.v +(1-B[1])*Gradient**2)/(1-B[1])\n",
    "\n",
    "\n",
    "        #In the end, update the parameters.\n",
    "        return self.params - self.alpha*self.m/(np.sqrt(self.v) + self.eps)\n",
    "\n",
    "    \n",
    "\n",
    "    def MainOptimalization(self, diff_bias:float):\n",
    "        value_change = float('inf')\n",
    "        \n",
    "        for i in range(250):\n",
    "            new_params = self.UpdateParams(np.array([0.4,0.4]))\n",
    "\n",
    "            self.params = new_params\n",
    "          \n",
    "        return self.params\n",
    "    \n",
    "\n",
    "                  \n",
    "\n",
    "\n",
    "\n",
    "Adamek2 = Adamv2(np.array([0.5, 0.5]), 0.01)\n",
    "\n",
    "print(Adamek2.MainOptimalization(0.01))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
